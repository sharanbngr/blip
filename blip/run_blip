#!/bin/env python3

import pickle
import numpy as np
import sys, os,configparser, subprocess
sys.path.append(os.getcwd()) ## this lets python find src
from blip.src.makeLISAdata import LISAdata
from blip.src.models import Model, Injection, ensure_color_matching
from blip.tools.plotmaker import plotmaker
from blip.tools.plotmaker import mapmaker
from blip.tools.plotmaker import fitmaker
import matplotlib.pyplot as plt
from multiprocessing import Pool
import time

class LISA(LISAdata, Model):

    '''
    Generic class for getting data and setting up the prior space
    and likelihood. This is tuned for ISGWB analysis at the moment
    but it should not be difficult to modify for other use cases.
    '''

    def __init__(self,  params, inj):
        
        # set up the LISAdata class
        LISAdata.__init__(self, params, inj)

        # Generate or get mldc or other loaded data
        if self.params['load_data']:
            self.process_external_data()
        elif self.inj['loadInj']:
            self.load_blip_injection_data()
        else:
            self.makedata()
        
        ## compute the cross/auto-channel correlations
        self.make_data_correlation_matrix()
        
        if not self.inj['inj_only']:
            # Set up the Bayes class
            print("Building Bayesian model...")
            self.Model = Model(params,inj,self.fdata,self.f0,self.tsegmid,self.rmat)
            
            # make sure matching injections/models have matching colors
            if not self.params['load_data']:
                ensure_color_matching(self.Model,self.Injection)
        
        # Make some simple diagnostic plots to contrast spectra
        if self.params['load_data']:
            self.plot_spectra()
        elif not self.inj['loadInj']:
            self.diag_spectra()
            
    def load_blip_injection_data(self):
        '''
        Function to load in data created via BLIP's own Injection routines, with associated Injection file.
        This is useful because we save true values, frozen spectra, etc. in addition to the raw data stream,
        so loading autogenerated data in this way allows BLIP's plotting and post-processing routines to
        return significantly more informative outputs.
        '''
        print("Loading extant BLIP-generated data from {}".format(self.inj['injdir']))
        
        with open(self.inj['injdir'] + '/injection.pickle', 'rb') as injectionfile:
            self.Injection = pickle.load(injectionfile)
        
        loaded_data = np.load(self.inj['injdir']+'/simulated_data.npz')
        
        self.timearray, self.h1, self.h2, self.h3 = loaded_data['timearray'], loaded_data['h1'], loaded_data['h2'], loaded_data['h3']
        
        # Generate lisa freq domain data from time domain data
        self.r1, self.r2, self.r3, self.fdata, self.tsegstart, self.tsegmid = self.tser2fser(self.h1, self.h2, self.h3, self.timearray)

        # Charactersitic frequency. Define f0
        cspeed = 3e8
        fstar = cspeed/(2*np.pi*self.armlength)
        self.f0 = self.fdata/(2*fstar)
        
    def makedata(self):

        '''
        Just a wrapper function to use the methods the LISAdata class
        to generate data. Return Frequency domain data.
        '''

        ## define the splice segment duration
        tsplice = 1e4
        ## the segments to be splices are half-overlapping
        nsplice = 2*int(self.params['dur']/tsplice) + 1
        ## arrays of segmnent start and mid times
        tsegmid = self.params['tstart'] +  (tsplice/2.0) * np.arange(nsplice) + (tsplice/2.0)
        ## Number of time-domain points in a splice segment
        Npersplice = int(self.params['fs']*tsplice)
        ## leave out f = 0
        frange = np.fft.rfftfreq(Npersplice, 1.0/self.params['fs'])[1:]
        ## the charecteristic frequency of LISA, and the scaled frequency array
        fstar = 3e8/(2*np.pi*self.armlength)
        f0 = frange/(2*fstar)
        
        ## Build the Injection object
        print("Constructing injection...")
        self.Injection = Injection(self.params,self.inj,frange,f0,tsegmid)
        
        ## assign a couple additional universal injection attributes needed in add_sgwb_data()
        self.Injection.Npersplice = Npersplice
        self.Injection.nsplice = nsplice
        
        # Generate TDI noise
        times, self.h1, self.h2, self.h3 = self.Injection.components['noise'].gen_noise_spectrum()
        delt = times[1] - times[0]

        # Cut to required size
        N = int((self.params['dur'])/delt)
        self.h1, self.h2, self.h3 = self.h1[0:N], self.h2[0:N], self.h3[0:N]

        ## create time-domain contribution from each injection component that isn't noise
        for component in self.Injection.sgwb_component_names:
            h1_gw, h2_gw, h3_gw, times = self.add_sgwb_data(self.Injection.components[component])
            h1_gw, h2_gw, h3_gw = h1_gw[0:N], h2_gw[0:N], h3_gw[0:N]

            # Add gravitational-wave time series to noise time-series
            self.h1 = self.h1 + h1_gw
            self.h2 = self.h2 + h2_gw
            self.h3 = self.h3 + h3_gw
        

        self.timearray = times[0:N]
        if delt != (times[1] - times[0]):
            raise ValueError('The noise and signal arrays are at different sampling frequencies!')

        # Desample if we increased the sample rate for time-shifts.
        if self.params['fs'] != 1.0/delt:
            self.params['fs'] = 1.0/delt

        # Generate lisa freq domain data from time domain data
        self.r1, self.r2, self.r3, self.fdata, self.tsegstart, self.tsegmid = self.tser2fser(self.h1, self.h2, self.h3, self.timearray)

        # Charactersitic frequency. Define f0
        cspeed = 3e8
        fstar = cspeed/(2*np.pi*self.armlength)
        self.f0 = self.fdata/(2*fstar)
        
    def make_data_correlation_matrix(self):
        '''
        Uses the generated time-domain data series to construct a data correlation matrix.
        
        Used to be the initialization of the (now defunct) likelihoods.py
        '''
        self.r12 = np.conj(self.r1)*self.r2
        self.r13 = np.conj(self.r1)*self.r3
        self.r21 = np.conj(self.r2)*self.r1
        self.r23 = np.conj(self.r2)*self.r3
        self.r31 = np.conj(self.r3)*self.r1
        self.r32 = np.conj(self.r3)*self.r2
        self.rbar = np.stack((self.r1, self.r2, self.r3), axis=2)

        ## create a data correlation matrix
        self.rmat = np.zeros((self.rbar.shape[0], self.rbar.shape[1], self.rbar.shape[2], self.rbar.shape[2]), dtype='complex')

        for ii in range(self.rbar.shape[0]):
            for jj in range(self.rbar.shape[1]):
                self.rmat[ii, jj, :, :] = np.tensordot(np.conj(self.rbar[ii, jj, :]), self.rbar[ii, jj, :], axes=0 )

    def diag_spectra(self):

        '''
        A function to do simple diagnostics. Plot the expected spectra and data.
        '''

        # ------------ Calculate PSD ------------------

        # PSD from the FFTs
        data_PSD1, data_PSD2, data_PSD3  = np.mean(np.abs(self.r1)**2, axis=1), np.mean(np.abs(self.r2)**2, axis=1), np.mean(np.abs(self.r3)**2, axis=1)

        # "Cut" to desired frequencies
        idx = np.logical_and(self.fdata >=  self.params['fmin'] , self.fdata <=  self.params['fmax'])
        psdfreqs = self.fdata[idx]

        #Charactersitic frequency
        fstar = 3e8/(2*np.pi*self.armlength)

        # define f0 = f/2f*
        f0 = self.fdata/(2*fstar)

        # Get desired frequencies for the PSD
        # We want to normalize PSDs to account for the windowing
        # Also convert from doppler-shift spectra to strain spectra
        data_PSD1,data_PSD2, data_PSD3 = data_PSD1[idx], data_PSD2[idx], data_PSD3[idx]

        # The last two elements are the position and the acceleration noise levels.
        Np, Na = 10**self.Injection.components['noise'].injvals['log_Np'], 10**self.Injection.components['noise'].injvals['log_Na']

        # Modelled Noise PSD
        C_noise = self.Injection.components['noise'].instr_noise_spectrum(self.fdata,self.f0, Np, Na)

        # Extract noise auto-power
        S1, S2, S3 = C_noise[0, 0, :], C_noise[1, 1, :], C_noise[2, 2, :]
        
        ## need to generate the population response at the data frequencies
        ## we can't do this earlier because we need the data time array
        if 'population' in self.Injection.sgwb_component_names:
            ## just use the analysis response if the lmax are the same (or there is no sph component)
            if not self.params['sph_flag'] and not self.inj['sph_flag']:
                ## grab a random isotropic response
                sm_name = [name for name in self.Model.submodel_names if name!='noise'][0]
                response_mat = self.Model.submodels[sm_name].response_mat
                self.Injection.components['population'].inj_response_mat_true = response_mat
            elif self.params['sph_flag'] and self.inj['sph_flag'] and self.inj['inj_lmax']==self.params['lmax']:
                ## grab a random anisotropic response
                sm_name = [name for name in self.Model.submodel_names if (name!='noise' and self.Model.submodels[name].has_map)][0]
                response_mat = self.Model.submodels[sm_name].response_mat
                self.Injection.components['population'].inj_response_mat_true = np.einsum('ijklm,m', response_mat, self.Injection.components['population'].alms_inj)
            else:
                inj_response_mat_true = self.Injection.components['population'].response(f0,self.tsegmid,**self.Injection.components['population'].response_kwargs)
                if hasattr(self.Injection.components['population'],'has_map') and self.Injection.components['population'].has_map:
                    self.Injection.components['population'].inj_response_mat_true = np.einsum('ijklm,m', inj_response_mat_true, self.Injection.components['population'].alms_inj)
                else:
                    self.Injection.components['population'].inj_response_mat_true = inj_response_mat_true

        ## compute and save the time-averaged response-convolved spectrum for plotting, etc.
        for cmn in self.Injection.sgwb_component_names:
            cm = self.Injection.components[cmn]
            if hasattr(cm,"ispop") and cm.ispop:
                Sgw_convolved = np.mean(cm.population.Sgw_true[None,None,:,None] * cm.inj_response_mat_true,axis=-1)
            else:
                Sgw_convolved = np.mean(cm.frozen_spectra[None,None,:,None] * cm.inj_response_mat,axis=-1)
            cm.frozen_convolved_spectra = Sgw_convolved

        plt.close()
        
        # noise budget plot
        plt.loglog(psdfreqs, data_PSD3,label='Simulated Data Series PSD', alpha=0.6, lw=0.75,color='slategrey')
        plt.loglog(self.fdata, C_noise[2, 2, :], label='Instrumental Noise ', lw=0.75,color='dimgrey')
        
        ymins = []
        for component_name in self.Injection.sgwb_component_names:
            S1_gw = self.Injection.plot_injected_spectra(component_name,fs_new=self.fdata,convolved=True,legend=True,channels='11',return_PSD=True,lw=0.75,color=self.Injection.components[component_name].color)
            ymins.append(S1_gw.min())
            S2_gw, S3_gw = self.Injection.compute_convolved_spectra(component_name,fs_new=self.fdata,channels='22'), self.Injection.compute_convolved_spectra(component_name,fs_new=self.fdata,channels='33')
            S1, S2, S3 = S1+S1_gw, S2+S2_gw, S3+S3_gw
        
        plt.loglog(self.fdata, S1, label='Simulated Total spectrum', lw=0.75,color='cadetblue')

        ## avoid plot squishing due to signal spectra with cutoffs, etc.
        if len(ymins) > 0:
            ymin = np.min(ymins)
            if ymin < 1e-43:
                plt.ylim(bottom=1e-43)
        
        plt.legend(loc='upper right')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.savefig(self.params['out_dir'] + '/psd_budget.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/psd_budget.png')
        plt.close()


        plt.loglog(self.fdata, S3, label='required',color='mediumvioletred')
        plt.loglog(psdfreqs, data_PSD3,label='PSD, data', alpha=0.6,color='slategrey')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.legend()
        plt.grid(linestyle=':',linewidth=0.5 )
        plt.xlim(self.params['fmin'], self.params['fmax'])

        plt.savefig(self.params['out_dir'] + '/diag_psd.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/diag_psd.png')
        plt.close()




        ## lets also plot psd residue.
        rel_res_mean = (data_PSD3 - S3)/S3

        plt.semilogx(self.fdata, rel_res_mean , label='relative mean residue',color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel(' Rel. residue')
        plt.ylim([-1.50, 1.50])
        plt.legend()
        plt.grid()
        plt.xlim(self.params['fmin'], self.params['fmax'])

        plt.savefig(self.params['out_dir'] + '/res_psd.png', dpi=200)
        print('Residue spectra plot made in ' + self.params['out_dir'] + '/res_psd.png')
        plt.close()
        
        # cross-power diag plots. We will only do 12. IF TDI=XYZ this is S_XY and if TDI=AET
        # this will be S_AE
        
        ii, jj = 2,0
        IJ = str(ii+1)+str(jj+1)
        
        Sx = C_noise[ii,jj,:]
        
        ymins = []
        iymins = []
        for component_name in self.Injection.sgwb_component_names:
            if component_name != 'noise':
                Sx_gw = self.Injection.compute_convolved_spectra(component_name,fs_new=self.fdata,channels=IJ) + self.Injection.compute_convolved_spectra(component_name,fs_new=self.fdata,channels=IJ,imaginary=True)
                ymins.append(np.real(Sx_gw).min())
                iymins.append(np.imag(Sx_gw).min())
                Sx = Sx + Sx_gw
       
        CSDx = np.mean(np.conj(self.rbar[:, :, ii]) * self.rbar[:, :, jj], axis=1)

        plt.subplot(2, 1, 1)
        if len(Sx.shape) == 1:
            plt.loglog(self.fdata, np.abs(np.real(Sx)), label='Re(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        else:
            plt.loglog(self.fdata, np.mean(np.abs(np.real(Sx)),axis=1), label='Re(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        plt.loglog(psdfreqs, np.abs(np.real(CSDx)) ,label='Re(CSD' + str(ii+1) + str(jj+1) + ')', alpha=0.6,color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel('Power in 1/Hz')
        plt.legend()
        plt.ylim([1e-44, 5e-40])
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.grid()

        plt.subplot(2, 1, 2)
        if len(Sx.shape) == 1:
            plt.loglog(self.fdata, np.abs(np.imag(Sx)), label='Im(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        else:
            plt.loglog(self.fdata, np.mean(np.abs(np.imag(Sx)),axis=1), label='Im(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')

        plt.loglog(psdfreqs, np.abs(np.imag(CSDx)) ,label='Im(CSD' + str(ii+1) + str(jj+1) + ')', alpha=0.6,color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel(' Power in 1/Hz')
        plt.legend()
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.ylim([1e-44, 5e-40])
        plt.grid()
        plt.savefig(self.params['out_dir'] + '/diag_csd_' + str(ii+1) + str(jj+1) + '.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/diag_csd_' + str(ii+1) + str(jj+1) + '.png')
        plt.close()
        
    def plot_spectra(self):
        '''
        A function to make a plot of the data spectrum. For use with external (non-autogenerated) data, where we cannot calculate the intrinsic components.
        '''
    
        # PSD from the FFTs
        data_PSD1, data_PSD2, data_PSD3  = np.mean(np.abs(self.r1)**2, axis=1), np.mean(np.abs(self.r2)**2, axis=1), np.mean(np.abs(self.r3)**2, axis=1)
    
        # "Cut" to desired frequencies
        idx = np.logical_and(self.fdata >=  self.params['fmin'] , self.fdata <=  self.params['fmax'])
        psdfreqs = self.fdata[idx]
    
        # Get desired frequencies for the PSD
        data_PSD1,data_PSD2, data_PSD3 = data_PSD1[idx], data_PSD2[idx], data_PSD3[idx]
        
        plt.loglog(psdfreqs, data_PSD1,label='PSD (1)', alpha=0.6, color='slategrey')
        plt.loglog(psdfreqs, data_PSD2,label='PSD (2)', alpha=0.6, color='rosybrown')
        plt.loglog(psdfreqs, data_PSD3,label='PSD (3)', alpha=0.6, color='mediumseagreen')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.legend()
        plt.grid(linestyle=':',linewidth=0.5 )
        plt.xlim(self.params['fmin'], self.params['fmax'])
    
        plt.savefig(self.params['out_dir'] + '/data_psd.png', dpi=200)
        print('Data spectra plot made in ' + self.params['out_dir'] + '/data_psd.png')
        plt.close()


def blip(paramsfile='params.ini',resume=False):
    '''
    The main workhorse of the bayesian pipeline.

    Input:
    Params File

    Output: Files containing evidence and pdfs of the parameters
    '''


    #  --------------- Read the params file --------------------------------

    # Initialize Dictionaries
    params = {}
    inj = {}

    config = configparser.ConfigParser()
    config.read(paramsfile)

    # Params Dict
    params['fmin']     = float(config.get("params", "fmin"))
    params['fmax']     = float(config.get("params", "fmax"))
    params['dur']      = float(config.get("params", "duration"))
    params['seglen']   = float(config.get("params", "seglen", fallback=1e5))
    params['fs']       = float(config.get("params", "fs", fallback=0.25))
    params['Shfile']   = config.get("params", "Shfile", fallback='LISA_2017_PSD_M.npy')
    params['load_data'] = int(config.get("params", "load_data", fallback=0))
    params['datatype'] = str(config.get("params", "datatype", fallback='strain'))
    params['datafile']  = str(config.get("params", "datafile", fallback=None))
    params['fref'] = float(config.get("params", "fref", fallback=25))
    
    params['model'] = str(config.get("params", "model"))
    
    ## get the model fixed values, passed as a dict
    fixedvals = eval(str(config.get("params","fixedvals",fallback='None')))
    
    params['fixedvals'] = {}
    if fixedvals is not None:
        ## enforce that the keys of the fixedvals dict correspond to the desired models
        ## at this point we will only make sure that all the keys are in the model string
        submodel_names = params['model'].split('+')
        if not np.all([key in submodel_names for key in fixedvals.keys()]):
            raise ValueError("Fixedvals dictionary has an invalid key. Fixedvals must be provided as a nested dictionary with top-level keys corresponding to the models specified in the 'model' parameter.")
        ## build the fixedvals dict
        ## some quantities we want to use as log values, so convert them
        log_list = ["Np","Na","omega0","fbreak","fcut","fscale"]
        for submodel_name in fixedvals.keys():
            params['fixedvals'][submodel_name] = {}
            for name in fixedvals[submodel_name].keys():
                if name in log_list:
                    new_name = "log_"+name
                    params['fixedvals'][submodel_name][new_name] = np.log10(fixedvals[submodel_name][name])
                else:
                    params['fixedvals'][submodel_name][name] = fixedvals[submodel_name][name]
    
    
    
    params['alias'] = eval(str(config.get("params","alias",fallback="{}")))

    params['tdi_lev'] = str(config.get("params", "tdi_lev", fallback='xyz'))
    params['lisa_config'] = str(config.get("params", "lisa_config", fallback='orbiting'))
    params['nside'] = int(config.get("params", "nside"))
    params['lmax'] = int(config.get("params", "lmax"))
    params['tstart'] = float(config.get("params", "tstart", fallback=0))

    ## see if we need to initialize the spherical harmonic subroutines
    sph_check = [sublist.split('-')[0].split('_')[-1] for sublist in params['model'].split('+')]

    # Injection Dict
    inj['doInj']         = int(config.get("inj", "doInj"))
    inj['loadInj']       = int(config.get("inj", "loadInj", fallback=0))
    inj['inj_only']      = int(config.get("inj", "inj_only", fallback=0))
    
    if inj['doInj']:
        ## first see if we are loading the injection
        if inj['loadInj']:
            if inj['inj_only']:
                raise ValueError("Both loadInj and inj_only flags are set to True. This won't accomplish anything...")
            inj['injdir'] = str(config.get("inj", "injdir"))
            ## get the already-generated injection dict
            with open(inj['injdir'] + '/config.pickle', 'rb') as paramfile:
                ## things are loaded from the pickle file in the same order they are put in
                loaded_params = pickle.load(paramfile)
                loaded_inj = pickle.load(paramfile)
            ## for this to work, all params that impact the data/response times/frequencies/types can't change
            ## but you can change e.g., recovery model, sampler, etc.
            required_immutable = ['fmin','fmax','dur','seglen','fs','nside','tstart','lisa_config','tdi_lev','datatype']
            requirements_violated = [requirement for requirement in required_immutable if params[requirement] != loaded_params[requirement] ]
            if len(requirements_violated)>0:
                raise ValueError("Loaded injection is incompatible with specified configuration due to mismatches in the following config settings: {}".format(requirements_violated))
            ## update the injection dictionary with the loaded one
            inj |= loaded_inj
            ## reset the top-level injection flags to their original state
            inj['loadInj'] = True
            inj['inj_only'] = False
        
        ## otherwise make a new one
        else:
            inj['injection'] = str(config.get("inj", "injection"))
            
            ## get the injection truevals, passed as a dict
            truevals = eval(str(config.get("inj","truevals")))
            ## enforce that the keys of the truevals dict correspond to the injected models
            ## at this point we will only make sure that all the keys are in the injection string
            ## (not all injections will have truevals; e.g., population injections)
            inj_component_names = inj['injection'].split('+')
            if not np.all([key in inj_component_names for key in truevals.keys()]):
                raise ValueError("Truevals dictionary has an invalid key. Truevals must be provided as a nested dictionary with top-level keys corresponding to the injections specified in the 'injection' parameter.")
            ## build the truevals dict
            ## some quantities we want to use as log values, so convert them
            inj['truevals'] = {}
            log_list = ["Np","Na","omega0","fbreak","fcut","fscale"]
            for component_name in truevals.keys():
                inj['truevals'][component_name] = {}
                for name in truevals[component_name].keys():
                    if name in log_list:
                        new_name = "log_"+name
                        inj['truevals'][component_name][new_name] = np.log10(truevals[component_name][name])
                    else:
                        inj['truevals'][component_name][name] = truevals[component_name][name]
        
        ## add injections to the spherical harmonic check if needed
        sph_check = sph_check + [sublist.split('-')[0].split('_')[-1] for sublist in inj['injection'].split('+')]
    
        
    ## pop out to set sph flags
    params['sph_flag'] = ('sph' in sph_check) or ('hierarchical' in sph_check)
    
    ## some final flag, injection parameter setting if we aren't loading the Injection directly
    if inj['doInj'] and not inj['loadInj']:
        inj['sph_flag'] = np.any([(item not in ['noise','isgwb']) for item in sph_check])
        inj['pop_flag'] = 'population' in sph_check
        
        if inj['sph_flag']:
            try:
                inj['inj_lmax'] = int(config.get("inj", "inj_lmax"))
            except configparser.NoOptionError as err:
                if params['sph_flag']:
                    print("Performing a spherical harmonic basis injection and inj_lmax has not been specified. Injection and recovery will use same lmax (lmax={}).".format(params['lmax']))
                    inj['inj_lmax'] = params['lmax']
                else:
                    print("You are trying to do a spherical harmonic injection, but have not specified lmax.")
                    if 'lmax' in params.keys():
                        print("Warning: using analysis lmax parameter for inj_lmax, but you are not performing a spherical harmonic analysis.")
                        inj['inj_lmax'] = params['lmax']
                    else:
                        raise err
        
        if inj['doInj'] and inj['pop_flag']:
            inj['popfile']     = str(config.get("inj","popfile",fallback=None))
            inj['SNRcut']  = float(config.get("inj","SNRcut", fallback=7))
            colnames = str(config.get("inj","columns",fallback=None))
            colnames = colnames.split(',')
            inj['columns'] = colnames
            delimiter = str(config.get("inj","delimiter",fallback=None))
            if delimiter == 'space':
                delimiter = ' '
            elif delimiter == 'tab':
                delimiter = '\t'
            inj['delimiter'] = delimiter


    # some run parameters
    params['out_dir']            = str(config.get("run_params", "out_dir"))

    params['doPreProc']          = int(config.get("run_params", "doPreProc", fallback=0))
    params['input_spectrum']     = str(config.get("run_params", "input_spectrum", fallback='data_spectrum.npz'))
    params['projection']         = str(config.get("run_params", "projection", fallback='E'))
    params['FixSeed']            = int(config.get("run_params", "FixSeed", fallback=0))
    if params['FixSeed']:
        params['seed']               = int(config.get("run_params", "seed"))
    nthread                      = int(config.get("run_params", "Nthreads", fallback=1))
    
    params['colormap']       = str(config.get("run_params", "colormap", fallback='magma'))    
    
    ## sampler selection
    params['sampler'] = str(config.get("run_params", "sampler"))
    
    ## sampler setup and late-time imports to reduce dependencies
    ## dynesty
    if params['sampler'] == 'dynesty':
        from blip.src.dynesty_engine import dynesty_engine
        nlive                   = int(config.get("run_params", "nlive", fallback=800))
        params['sample_method'] = str(config.get("run_params", "sample_method", fallback='rwalk'))
    # nessai
    elif params['sampler'] == 'nessai':
        from blip.src.nessai_engine import nessai_engine
        nlive                        = int(config.get("run_params", "nlive", fallback=2000))
        ## flow tuning
        params['nessai_neurons']     = str(config.get("run_params", "nessai_neurons", fallback='scale_greedy'))
        if params['nessai_neurons']=='manual':
            params['n_neurons']      = int(config.get("run_params", "n_neurons", fallback=20))
        params['reset_flow']         = int(config.get("run_params", "reset_flow", fallback=16))
    ## emcee
    elif params['sampler'] == 'emcee':
        from blip.src.emcee_engine import emcee_engine
        params['Nburn'] = int(config.get("run_params", "Nburn",fallback=1000))
        params['Nsamples'] = int(config.get("run_params", "Nsamples",fallback=1000))
    ## numpyro
    elif params['sampler'] == 'numpyro':
        if nthread > 1:
            os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count={}'.format(nthread)
        from blip.src.numpyro_engine import numpyro_engine
        params['show_progress'] = int(config.get("run_params", "show_progress", fallback=1))
        params['Nburn'] = int(config.get("run_params", "Nburn",fallback=1000))
        params['Nsamples'] = int(config.get("run_params", "Nsamples",fallback=1000))
    else:
        raise ValueError("Unknown sampler. Can be 'dynesty', 'emcee', 'numpyro', or 'nessai' for now.")
    # checkpointing (dynesty+nessai only for now)
    if params['sampler']=='dynesty' or params['sampler'] == 'nessai' or params['sampler'] == 'numpyro':
        params['checkpoint']            = int(config.get("run_params", "checkpoint", fallback=0))
        ## numpyro's checkpoint_interval is in number of samples, vs. seconds for dynesty/nessai
        if params['sampler'] == 'numpyro':
            params['checkpoint_interval']   = int(config.get("run_params", "checkpoint_interval", fallback=100))
        else:
            params['checkpoint_interval']   = int(config.get("run_params", "checkpoint_interval", fallback=3600))

    # Fix random seed
    if params['FixSeed']:
        from blip.tools.SetRandomState import SetRandomState as setrs
        seed = params['seed']
        randst = setrs(seed)
    else:
        if params['checkpoint']:
            raise TypeError("Checkpointing without a fixed seed is not supported. Set 'FixSeed' to true and specify 'seed'.")
        if resume:
            raise TypeError("Resuming from a checkpoint requires re-generation of data, so the random seed MUST be fixed.")
        randst = None

    if not resume:
        # Make directories, copy stuff
        # Make output folder
        subprocess.call(["mkdir", "-p", params['out_dir']])
    
        # Copy the params file to outdir, to keep track of the parameters of each run.
        subprocess.call(["cp", paramsfile, params['out_dir']])
        
        # Initialize lisa class
        lisa =  LISA(params, inj)
        
        ## save the Model and Injection as needed
        
        ## the Injection is massive, so discard the responses we no longer need
        ## saving & discarding now, as opposed to at the end of the run also saves space in the dynesty/nessai checkpoint files.
        if not params['load_data']:
            ## save Injection
            for cmn in lisa.Injection.component_names:
                if hasattr(lisa.Injection.components[cmn],'response_mat'):
                    del lisa.Injection.components[cmn].response_mat
                if hasattr(lisa.Injection.components[cmn],'summ_response_mat'):
                    del lisa.Injection.components[cmn].summ_response_mat
                if hasattr(lisa.Injection.components[cmn],'inj_response_mat'):
                    del lisa.Injection.components[cmn].inj_response_mat
            with open(params['out_dir'] + '/injection.pickle', 'wb') as outfile:
                pickle.dump(lisa.Injection, outfile)
            print("Saved Injection to "+params['out_dir']+"/injection.pickle")
            
            ## save generated data
            np.savez_compressed(params['out_dir']+'/simulated_data.npz',timearray=lisa.timearray,h1=lisa.h1,h2=lisa.h2,h3=lisa.h3)
            print("Saved strain time series to "+params['out_dir']+"/simulated_data.npz")
            ## Data generation is complete. Exit if only performing injection and data simulation.
            if inj['inj_only']:
                print("Simulation and generation of LISA data is complete and inj_only flag is set to True. Saving configuration and exiting...")
                # Save parameters as a pickle
                with open(params['out_dir'] + '/config.pickle', 'wb') as outfile:
                    pickle.dump(params, outfile)
                    pickle.dump(inj, outfile)
                
                return
        
        ## the Model tends to be more lightweight
        with open(params['out_dir'] + '/model.pickle', 'wb') as outfile:
            pickle.dump(lisa.Model, outfile)
        print("Saved Model to "+params['out_dir']+"/model.pickle")

        print("Generating sampling engine...")
    else:
        print("Resuming a previous analysis. Reloading data and sampler state...")

    if params['sampler'] == 'dynesty':
        # Create engine
        if not resume:
            # multiprocessing
            if nthread > 1:
                pool = Pool(nthread)
            else:
                pool = None
            engine, parameters = dynesty_engine().define_engine(lisa, params, nlive, nthread, randst, pool=pool)    
        else:
            pool = None
            if nthread > 1:
                print("Warning: Nthread > 1, but multiprocessing is not supported when resuming a run. Pool set to None.")
                ## To anyone reading this and wondering why:
                ## The pickle calls used by Python's multiprocessing fail when trying to run the sampler after saving/reloading it.
                ## This is because pickling the sampler maps all its attributes to their full paths;
                ## e.g., dynesty_engine.isgwb_prior is named as src.dynesty_engine.dynesty_engine.isgwb_prior
                ## BUT the object itself is still e.g. <function dynesty_engine.isgwb_prior at 0x7f8ebcc27130>
                ## so we get an error like
                ## _pickle.PicklingError: Can't pickle <function dynesty_engine.isgwb_prior at 0x7f8ebcc27130>: \
                ##                        it's not the same object as src.dynesty_engine.dynesty_engine.isgwb_prior
                ## See e.g. https://stackoverflow.com/questions/1412787/picklingerror-cant-pickle-class-decimal-decimal-its-not-the-same-object
                ## After too much time and sanity spent trying to fix this, I have admitted defeat.
                ## Feel free to try your hand -- maybe you're the chosen one. Good luck.
                
            engine, parameters = dynesty_engine.load_engine(params,randst,pool)
        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples, logz, logzerr = dynesty_engine.run_engine_with_checkpointing(engine,parameters,params['checkpoint_interval'],checkpoint_file)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        else:
            t1 = time.time()
            post_samples, logz, logzerr = dynesty_engine.run_engine(engine)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        if nthread > 1:
            engine.pool.close()
            engine.pool.join()
        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
        np.savetxt(params['out_dir'] + "/logz.txt", logz)
        np.savetxt(params['out_dir'] + "/logzerr.txt", logzerr)

    elif params['sampler'] == 'emcee':
        # multiprocessing
        if nthread>1:
            pool=Pool(nthread)
        else:
            pool=None

        # Create engine
        engine, parameters, init_samples = emcee_engine.define_engine(lisa.Model, nlive, randst, pool=pool)
        unit_samples, post_samples = emcee_engine.run_engine(engine, lisa.Model, init_samples,params['Nburn'],params['Nsamples'])

        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/unit_samples.txt",unit_samples)
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)

    elif params['sampler'] == 'numpyro':
        ## create engine
        if not resume:
            ## without checkpointing, set up the engine to run for Nsamples samples
            ## with checkpointing, set it up to run for checkpoint_interval samples (and set the starting chain to None)
            if params['checkpoint']:
                Nrun = params['checkpoint_interval']
                chain = None
            else:
                Nrun = params['Nsamples']
                
            engine, parameters, rng_key = numpyro_engine.define_engine(lisa, params['Nburn'], Nrun, nthread, params['show_progress'], params['seed'])
            lisaModel = lisa.Model
        else:
            resume_file = params['out_dir']+'/checkpoint.pickle'
            engine, rng_key, chain = numpyro_engine.load_engine(resume_file)
            with open(params['out_dir'] + '/model.pickle', 'rb') as modfile:
                lisaModel = pickle.load(modfile)
            parameters = lisaModel.parameters

        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples = numpyro_engine.run_engine_with_checkpointing(engine,lisaModel,rng_key,chain,checkpoint_file,params['Nsamples'])
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        else:
            post_samples = numpyro_engine.run_engine(engine,lisa,rng_key)
        
        ## save chain
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
    
    elif params['sampler'] == 'nessai':
        # Create engine
        if not resume:
            engine, parameters, model = nessai_engine().define_engine(lisa, params, nlive, nthread, params['seed'], params['out_dir']+'/nessai_output/',checkpoint_interval=params['checkpoint_interval'])    
        else:
            engine, parameters, model = nessai_engine.load_engine(params,nlive,nthread,params['seed'],params['out_dir']+'/nessai_output/',checkpoint_interval=params['checkpoint_interval'])
        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples, logz, logzerr = nessai_engine.run_engine_with_checkpointing(engine,parameters,model,params['out_dir']+'/nessai_output/',checkpoint_file)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
            np.savetxt(params['out_dir']+'/time_elapsed.txt',np.array([t2-t1]))
        else:
            t1 = time.time()
            post_samples, logz, logzerr = nessai_engine.run_engine(engine,parameters,model,params['out_dir']+'/nessai_output/')
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
            np.savetxt(params['out_dir']+'/time_elapsed.txt',np.array([t2-t1]))

        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
#        np.savetxt(params['out_dir'] + "/logz.txt", logz)
#        np.savetxt(params['out_dir'] + "/logzerr.txt", logzerr)
    
    else:
        raise TypeError('Unknown sampler model chosen. Only dynesty, nessai, numpyro, & emcee are supported')
    
    ## reload the Model and Injection if needed
    if not resume:
        plotModel, plotInjection = lisa.Model, lisa.Injection
    else:
        with open(params['out_dir'] + '/model.pickle', 'rb') as modfile:
            plotModel = pickle.load(modfile)
        with open(params['out_dir'] + '/injection.pickle', 'rb') as injfile:
            plotInjection = pickle.load(injfile)
    
    # Save parameters as a pickle
    with open(params['out_dir'] + '/config.pickle', 'wb') as outfile:
        pickle.dump(params, outfile)
        pickle.dump(inj, outfile)
        pickle.dump(parameters, outfile)
       
    print("\nMaking posterior Plots ...")
    if not params['load_data']:
        plotmaker(post_samples, params, parameters, inj, plotModel, plotInjection)
    else:
        plotmaker(post_samples, params, parameters, inj, plotModel)
    if not params['load_data']:
        fitmaker(post_samples, params, parameters, inj, plotModel, plotInjection)
    else:
        fitmaker(post_samples, params, parameters, inj, plotModel)
    ## make a map if there is a map to be made
    if np.any([plotModel.submodels[sm_name].has_map for sm_name in plotModel.submodel_names]):
        if 'healpy_proj' in params.keys():
            mapmaker(post_samples, params, parameters, plotModel,  coord=params['healpy_proj'])
        else:
            mapmaker(post_samples, params, parameters, plotModel)
        
    

if __name__ == "__main__":

    if len(sys.argv) != 2:
        if sys.argv[2] == 'resume':
            blip(sys.argv[1],resume=True)
        else:
            raise ValueError('Provide (only) the params file as an argument')
    else:
        blip(sys.argv[1])
