#!/bin/env python3

import pickle
import numpy as np
import sys, os, shutil, configparser, subprocess
sys.path.append(os.getcwd()) ## this lets python find src
from blip.src.makeLISAdata import LISAdata
from blip.src.models import Model, Injection
from blip.src.utils import ensure_color_matching
from blip.tools.plotmaker import cornermaker
from blip.tools.plotmaker import mapmaker
from blip.tools.plotmaker import fitmaker
import matplotlib.pyplot as plt
from multiprocessing import Pool
import time

class LISA(LISAdata, Model):

    '''
    Generic class for getting data and setting up the prior space
    and likelihood. This is tuned for ISGWB analysis at the moment
    but it should not be difficult to modify for other use cases.
    '''

    def __init__(self,  params, inj):
        
        # set up the LISAdata class
        LISAdata.__init__(self, params, inj)

        # Generate or get mldc or other loaded data
        if self.params['load_data']:
            self.process_external_data()
        elif self.inj['loadInj']:
            self.load_blip_injection_data()
        else:
            self.makedata()
        
        ## compute the cross/auto-channel correlations
        self.make_data_correlation_matrix()
        
        if not self.inj['inj_only']:
            # Set up the Bayes class
            print("Building Bayesian model...")
            self.Model = Model(params,inj,self.fdata,self.f0,self.tsegmid,self.rmat)
            
            # make sure matching injections/models have matching colors
            if not self.params['load_data']:
                ensure_color_matching(self.Model,self.Injection)
        
        ## if doing an injection, we also need the responses evaluated at the data frequencies for plotting/validation
        if not self.params['load_data'] and not self.inj['loadInj']:
            self.generate_inj_fdata_responses()
            self.compute_inj_fdata_spectra()
        
        # Make some simple diagnostic plots to contrast spectra
        if self.params['load_data']:
            self.plot_spectra()
        elif not self.inj['loadInj']:
            self.diag_spectra()
            
    def load_blip_injection_data(self):
        '''
        Function to load in data created via BLIP's own Injection routines, with associated Injection file.
        This is useful because we save true values, frozen spectra, etc. in addition to the raw data stream,
        so loading autogenerated data in this way allows BLIP's plotting and post-processing routines to
        return significantly more informative outputs.
        '''
        print("Loading extant BLIP-generated data from {}".format(self.inj['injdir']))
        
        with open(self.inj['injdir'] + '/injection.pickle', 'rb') as injectionfile:
            self.Injection = pickle.load(injectionfile)
        
        loaded_data = np.load(self.inj['injdir']+'/simulated_data.npz')
        
        self.timearray, self.h1, self.h2, self.h3 = loaded_data['timearray'], loaded_data['h1'], loaded_data['h2'], loaded_data['h3']
        
        # Generate lisa freq domain data from time domain data
        self.r1, self.r2, self.r3, self.fdata, self.tsegstart, self.tsegmid = self.tser2fser(self.h1, self.h2, self.h3, self.timearray)

        # Charactersitic frequency. Define f0
        cspeed = 3e8
        fstar = cspeed/(2*np.pi*self.armlength)
        self.f0 = self.fdata/(2*fstar)
        
    def makedata(self):

        '''
        Just a wrapper function to use the methods the LISAdata class
        to generate data. Return Frequency domain data.
        '''

        ## define the splice segment duration
        tsplice = 1e4
        ## the segments to be splices are half-overlapping
        nsplice = 2*int(self.params['dur']/tsplice) + 1
        ## arrays of segmnent start and mid times
        tsegmid = self.params['tstart'] +  (tsplice/2.0) * np.arange(nsplice) + (tsplice/2.0)
        ## Number of time-domain points in a splice segment
        Npersplice = int(self.params['fs']*tsplice)
        ## leave out f = 0
        frange = np.fft.rfftfreq(Npersplice, 1.0/self.params['fs'])[1:]
        ## the charecteristic frequency of LISA, and the scaled frequency array
        fstar = 3e8/(2*np.pi*self.armlength)
        f0 = frange/(2*fstar)
        
        ## Build the Injection object
        print("Constructing injection...")
        self.Injection = Injection(self.params,self.inj,frange,f0,tsegmid)
        
        ## assign a couple additional universal injection attributes needed in add_sgwb_data()
        self.Injection.Npersplice = Npersplice
        self.Injection.nsplice = nsplice
        
        # Generate TDI noise
        times, self.h1, self.h2, self.h3 = self.Injection.components['noise'].gen_noise_spectrum()
        delt = times[1] - times[0]

        # Cut to required size
        N = int((self.params['dur'])/delt)
        self.h1, self.h2, self.h3 = self.h1[0:N], self.h2[0:N], self.h3[0:N]

        ## create time-domain contribution from each injection component that isn't noise
        for component in self.Injection.sgwb_component_names:
            h1_gw, h2_gw, h3_gw, times = self.add_sgwb_data(self.Injection.components[component])
            h1_gw, h2_gw, h3_gw = h1_gw[0:N], h2_gw[0:N], h3_gw[0:N]

            # Add gravitational-wave time series to noise time-series
            self.h1 = self.h1 + h1_gw
            self.h2 = self.h2 + h2_gw
            self.h3 = self.h3 + h3_gw
        

        self.timearray = times[0:N]
        if delt != (times[1] - times[0]):
            raise ValueError('The noise and signal arrays are at different sampling frequencies!')

        # Desample if we increased the sample rate for time-shifts.
        if self.params['fs'] != 1.0/delt:
            self.params['fs'] = 1.0/delt

        # Generate lisa freq domain data from time domain data
        self.r1, self.r2, self.r3, self.fdata, self.tsegstart, self.tsegmid = self.tser2fser(self.h1, self.h2, self.h3, self.timearray)

        # Charactersitic frequency. Define f0
        cspeed = 3e8
        fstar = cspeed/(2*np.pi*self.armlength)
        self.f0 = self.fdata/(2*fstar)
        
    def make_data_correlation_matrix(self):
        '''
        Uses the generated time-domain data series to construct a data correlation matrix.
        
        Used to be the initialization of the (now defunct) likelihoods.py
        '''
        self.r12 = np.conj(self.r1)*self.r2
        self.r13 = np.conj(self.r1)*self.r3
        self.r21 = np.conj(self.r2)*self.r1
        self.r23 = np.conj(self.r2)*self.r3
        self.r31 = np.conj(self.r3)*self.r1
        self.r32 = np.conj(self.r3)*self.r2
        self.rbar = np.stack((self.r1, self.r2, self.r3), axis=2)

        ## create a data correlation matrix
        self.rmat = np.zeros((self.rbar.shape[0], self.rbar.shape[1], self.rbar.shape[2], self.rbar.shape[2]), dtype='complex')

        for ii in range(self.rbar.shape[0]):
            for jj in range(self.rbar.shape[1]):
                self.rmat[ii, jj, :, :] = np.tensordot(np.conj(self.rbar[ii, jj, :]), self.rbar[ii, jj, :], axes=0 )

    def generate_inj_fdata_responses(self):
        '''
        A function to generate the response functions for each injection at the simulated data frequencies.
        
        This is needed to allow us to accurately plot and compare injected spectra vs. simulated data vs. recovered models.
        
        '''
        ## only do this for gw signals (i.e., not the noise)
        for cmn in self.Injection.sgwb_component_names:
            
            cm = self.Injection.components[cmn]
            
            #Charactersitic frequency
            fstar = 3e8/(2*np.pi*self.armlength)
            # define f0 = f/2f*
            f0_data = self.fdata/(2*fstar)

            ## the response calculations at data frequencies are very cheap, so the (large) memory transfer time to make copies for multiprocessing is not worth the trouble
            if hasattr(cm,"response_non_parallel"):
                fdata_response = cm.response_non_parallel(f0_data,self.tsegmid,**cm.response_kwargs)
            else:
                fdata_response = cm.response(f0_data,self.tsegmid,**cm.response_kwargs)
            ## need to convolve response with skymap for sph basis anisotropic signals
            ## this is done implciitly for pixel-basis injections
            if cm.has_map and cm.basis == 'sph':
                summ_fdata_response = np.einsum('ijklm,m', fdata_response, cm.alms_inj)
                cm.fdata_response_mat = summ_fdata_response
            else:
                cm.fdata_response_mat = fdata_response
        
        return
    
    def compute_inj_fdata_spectra(self):
        '''
        A function to compute the injected spectra as evaluated at the simulated data frequencies.
        
        This is needed to allow us to accurately plot and compare injected spectra vs. simulated data vs. recovered models.
        '''
        for cmn in self.Injection.component_names:
            
            cm = self.Injection.components[cmn]
            
            #Charactersitic frequency
            fstar = 3e8/(2*np.pi*self.armlength)
            # define f0 = f/2f*
            f0_data = self.fdata/(2*fstar)
            cm.fdata = self.fdata
            ## note that for astrophysical signals we take the time-average but keep the full 3x3 correlation matrix
            if cmn == 'noise':
                Np, Na = 10**cm.injvals['log_Np'], 10**cm.injvals['log_Na']
                cm.fdata_noise = cm.instr_noise_spectrum(self.fdata,f0_data, Np, Na)
            elif hasattr(cm,"ispop") and cm.ispop:
                cm.fdata_spectra = cm.population.Sgw_true
                cm.fdata_convolved_spectra = np.mean(cm.fdata_spectra[None,None,:,None] * cm.fdata_response_mat,axis=-1)
            else:
                spec_args = [cm.truevals[parameter] for parameter in cm.spectral_parameters]
                cm.fdata_spectra = cm.compute_Sgw(self.fdata,spec_args)
                cm.fdata_convolved_spectra = np.mean(cm.fdata_spectra[None,None,:,None] * cm.fdata_response_mat,axis=-1)
        
        return
                
            
        
                
            
    
    def diag_spectra(self):

        '''
        A function to do simple diagnostics. Plot the expected spectra and data.
        '''

        # ------------ Calculate PSD ------------------

        # PSD from the FFTs
        data_PSD1, data_PSD2, data_PSD3  = np.mean(np.abs(self.r1)**2, axis=1), np.mean(np.abs(self.r2)**2, axis=1), np.mean(np.abs(self.r3)**2, axis=1)

        # "Cut" to desired frequencies
        idx = np.logical_and(self.fdata >=  self.params['fmin'] , self.fdata <=  self.params['fmax'])
        psdfreqs = self.fdata[idx]

        # Get desired frequencies for the PSD
        # We want to normalize PSDs to account for the windowing
        # Also convert from doppler-shift spectra to strain spectra
        data_PSD1,data_PSD2, data_PSD3 = data_PSD1[idx], data_PSD2[idx], data_PSD3[idx]
        
        ## Get the noise component
        cmn_noise = self.Injection.components['noise']
        C_noise = cmn_noise.fdata_noise

        # Extract noise auto-power
        S1, S2, S3 = C_noise[0, 0, :], C_noise[1, 1, :], C_noise[2, 2, :]

        ## compute and save the time-averaged response-convolved spectrum for plotting, etc.
        for cmn in self.Injection.sgwb_component_names:
            cm = self.Injection.components[cmn]
            if hasattr(cm,"ispop") and cm.ispop:
                Sgw_convolved = cm.fdata_convolved_spectra ## the population spectra needs to be binned at the observed (data) frequencies in any case
            else:
                Sgw_convolved = np.mean(cm.frozen_spectra[None,None,:,None] * cm.inj_response_mat,axis=-1)
            cm.frozen_convolved_spectra = Sgw_convolved
        
        ## prep saving original PSDs
        ## load or create plot_data dict
        plot_data_path = self.params['out_dir']+'/plot_data.pickle'
        if os.path.exists(plot_data_path):
            with open(plot_data_path, 'rb') as datafile:
                plot_data = pickle.load(datafile)
                if 'injspec_data' not in plot_data.keys():
                    plot_data['injspec_data'] = {}
        else:
            plot_data = {'injspec_data':{}}
        
        plt.close()
        
        # noise budget plot
        ffilt = (self.fdata>=self.params['fmin'])*(self.fdata<=self.params['fmax'])
        plt.loglog(psdfreqs, data_PSD1,label='Simulated Data Series PSD', alpha=0.6, lw=0.75,color='slategrey')
        plt.loglog(self.fdata[ffilt], C_noise[2, 2, :][ffilt], label='Instrumental Noise ', lw=0.75,color='dimgrey')
        
        ymins = []
        ymeds = []
        ywmeds = []
        ydevs = []
        ylim_flag = False
        for component_name in self.Injection.sgwb_component_names:
            S1_gw = self.Injection.plot_injected_spectra(component_name,fs_new='data',convolved=True,legend=True,channels='11',return_PSD=True,lw=0.75,color=self.Injection.components[component_name].color)
            S1_gw_filt = S1_gw[ffilt]
            plot_data['injspec_data'][component_name] = {'S1_gw':S1_gw_filt}
            plot_data['injspec_data'][component_name]['f_filt'] = self.fdata[ffilt]
            log_S1_gw = np.log10(S1_gw_filt[np.nonzero(S1_gw_filt)])
            log_bin_sizes = np.log10(self.fdata[ffilt][np.nonzero(S1_gw_filt)])[1:] - np.log10(self.fdata[ffilt][np.nonzero(S1_gw_filt)])[:-1]
            ave_bin_val = (log_S1_gw[1:] + log_S1_gw[:-1])/2
            weighted_ymed = np.sum(log_bin_sizes*ave_bin_val/np.sum(log_bin_sizes))
            ymins.append(np.min(log_S1_gw))
            ymeds.append(np.median(log_S1_gw))
            ywmeds.append(weighted_ymed)
            ydevs.append(np.std(log_S1_gw))
            ylim_flag = True
            S2_gw, S3_gw = self.Injection.compute_convolved_spectra(component_name,fs_new='data',channels='22'), self.Injection.compute_convolved_spectra(component_name,fs_new='data',channels='33')
            S1, S2, S3 = S1+S1_gw, S2+S2_gw, S3+S3_gw
        
        plt.loglog(self.fdata[ffilt], S1[ffilt], label='Simulated Total spectrum', lw=0.75,color='cadetblue')

        ## set plot limits, with dynamic scaling for the y-axis to handle spectra with cutoffs, etc.
        if ylim_flag:
            ylows = [ywmed_i - ydev_i for ywmed_i,ydev_i in zip(ywmeds,ydevs)]
            ylow_min = np.min(ylows)
            plt.ylim(bottom=10**(ylow_min-1))
            ## save for plotting the results with plotmaker
            self.Injection.plot_ymin = 10**(ylow_min-1)

        plt.legend(loc='upper right')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.savefig(self.params['out_dir'] + '/psd_budget.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/psd_budget.png')
        plt.close()


        plt.loglog(self.fdata, S3, label='required',color='mediumvioletred')
        plt.loglog(psdfreqs, data_PSD3,label='PSD, data', alpha=0.6,color='slategrey')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.legend()
        plt.grid(linestyle=':',linewidth=0.5 )
        plt.xlim(self.params['fmin'], self.params['fmax'])
        ## avoid plot squishing due to signal spectra with cutoffs, etc.
        if len(ymins) > 0:
            ymin = np.min(ymins)
            if ymin < 1e-43:
                plt.ylim(bottom=1e-43)
        
        plt.savefig(self.params['out_dir'] + '/diag_psd.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/diag_psd.png')
        plt.close()


        ## lets also plot psd residue.        
        rel_res_mean = (data_PSD3 - S3)/S3

        plt.semilogx(self.fdata, rel_res_mean , label='relative mean residue',color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel(' Rel. residue')
        plt.ylim([-1.50, 1.50])
        plt.legend()
        plt.grid()
        plt.xlim(self.params['fmin'], self.params['fmax'])

        plt.savefig(self.params['out_dir'] + '/res_psd.png', dpi=200)
        print('Residue spectra plot made in ' + self.params['out_dir'] + '/res_psd.png')
        plt.close()
#        
        # cross-power diag plots. We will only do 12. IF TDI=XYZ this is S_XY and if TDI=AET
        # this will be S_AE
        
        ii, jj = 2,0
        IJ = str(ii+1)+str(jj+1)
        
        Sx = C_noise[ii,jj,:]
        
        ymins = []
        iymins = []
        for component_name in self.Injection.sgwb_component_names:
            if component_name != 'noise':
                Sx_gw = self.Injection.compute_convolved_spectra(component_name,fs_new='data',channels=IJ) + self.Injection.compute_convolved_spectra(component_name,fs_new='data',channels=IJ,imaginary=True)
                ymins.append(np.real(Sx_gw).min())
                iymins.append(np.imag(Sx_gw).min())
                Sx = Sx + Sx_gw
       
        CSDx = np.mean(np.conj(self.rbar[:, :, ii]) * self.rbar[:, :, jj], axis=1)

        plt.subplot(2, 1, 1)
        if len(Sx.shape) == 1:
            plt.loglog(self.fdata, np.abs(np.real(Sx)), label='Re(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        else:
            plt.loglog(self.fdata, np.mean(np.abs(np.real(Sx)),axis=1), label='Re(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        plt.loglog(psdfreqs, np.abs(np.real(CSDx)) ,label='Re(CSD' + str(ii+1) + str(jj+1) + ')', alpha=0.6,color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel('Power in 1/Hz')
        plt.legend()
        plt.ylim([1e-44, 5e-40])
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.grid()

        plt.subplot(2, 1, 2)
        if len(Sx.shape) == 1:
            plt.loglog(self.fdata, np.abs(np.imag(Sx)), label='Im(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')
        else:
            plt.loglog(self.fdata, np.mean(np.abs(np.imag(Sx)),axis=1), label='Im(Required ' + str(ii+1) + str(jj+1) + ')',color='mediumvioletred')

        plt.loglog(psdfreqs, np.abs(np.imag(CSDx)) ,label='Im(CSD' + str(ii+1) + str(jj+1) + ')', alpha=0.6,color='slategrey')
        plt.xlabel('f in Hz')
        plt.ylabel(' Power in 1/Hz')
        plt.legend()
        plt.xlim(self.params['fmin'], self.params['fmax'])
        plt.ylim([1e-44, 5e-40])
        plt.grid()
        plt.savefig(self.params['out_dir'] + '/diag_csd_' + str(ii+1) + str(jj+1) + '.png', dpi=200)
        print('Diagnostic spectra plot made in ' + self.params['out_dir'] + '/diag_csd_' + str(ii+1) + str(jj+1) + '.png')
        plt.close()
        
        
        ## save fit data
        if os.path.exists(plot_data_path):
            ## move to temp file
            temp_file = plot_data_path + ".temp"
            with open(temp_file, "wb") as datafile:
                pickle.dump(plot_data,datafile)
            shutil.move(temp_file, plot_data_path)
        else:
            with open(plot_data_path, 'wb') as datafile:
                plot_data = pickle.dump(plot_data,datafile)
        
    def plot_spectra(self):
        '''
        A function to make a plot of the data spectrum. For use with external (non-autogenerated) data, where we cannot calculate the intrinsic components.
        '''
    
        # PSD from the FFTs
        data_PSD1, data_PSD2, data_PSD3  = np.mean(np.abs(self.r1)**2, axis=1), np.mean(np.abs(self.r2)**2, axis=1), np.mean(np.abs(self.r3)**2, axis=1)
    
        # "Cut" to desired frequencies
        idx = np.logical_and(self.fdata >=  self.params['fmin'] , self.fdata <=  self.params['fmax'])
        psdfreqs = self.fdata[idx]
    
        # Get desired frequencies for the PSD
        data_PSD1,data_PSD2, data_PSD3 = data_PSD1[idx], data_PSD2[idx], data_PSD3[idx]
        
        plt.loglog(psdfreqs, data_PSD1,label='PSD (1)', alpha=0.6, color='slategrey')
        plt.loglog(psdfreqs, data_PSD2,label='PSD (2)', alpha=0.6, color='rosybrown')
        plt.loglog(psdfreqs, data_PSD3,label='PSD (3)', alpha=0.6, color='mediumseagreen')
        plt.xlabel('$f$ in Hz')
        plt.ylabel('PSD 1/Hz ')
        plt.legend()
        plt.grid(linestyle=':',linewidth=0.5 )
        plt.xlim(self.params['fmin'], self.params['fmax'])
    
        plt.savefig(self.params['out_dir'] + '/data_psd.png', dpi=200)
        print('Data spectra plot made in ' + self.params['out_dir'] + '/data_psd.png')
        plt.close()


def blip(paramsfile='params.ini',resume=False):
    '''
    The main workhorse of the bayesian pipeline.

    Input:
    Params File

    Output: Files containing evidence and pdfs of the parameters
    '''


    #  --------------- Read the params file --------------------------------

    # Initialize Dictionaries
    params = {}
    inj = {}

    config = configparser.ConfigParser()
    config.read(paramsfile)

    # Params Dict
    params['fmin']     = float(config.get("params", "fmin"))
    params['fmax']     = float(config.get("params", "fmax"))
    params['dur']      = float(config.get("params", "duration"))
    params['seglen']   = float(config.get("params", "seglen", fallback=1e5))
    params['fs']       = float(config.get("params", "fs", fallback=0.25))
    params['Shfile']   = config.get("params", "Shfile", fallback='LISA_2017_PSD_M.npy')
    params['load_data'] = int(config.get("params", "load_data", fallback=0))
    params['datatype'] = str(config.get("params", "datatype", fallback='strain'))
    params['datafile']  = str(config.get("params", "datafile", fallback=None))
    params['fref'] = float(config.get("params", "fref", fallback=25))
    
    params['model'] = str(config.get("params", "model"))
    
    ## get the model fixed values, passed as a dict
    fixedvals = eval(str(config.get("params","fixedvals",fallback='None')))
    
    params['fixedvals'] = {}
    if fixedvals is not None:
        ## enforce that the keys of the fixedvals dict correspond to the desired models
        ## at this point we will only make sure that all the keys are in the model string
        submodel_names = params['model'].split('+')
        if not np.all([key in submodel_names for key in fixedvals.keys()]):
            raise ValueError("Fixedvals dictionary has an invalid key. Fixedvals must be provided as a nested dictionary with top-level keys corresponding to the models specified in the 'model' parameter.")
        ## build the fixedvals dict
        ## some quantities we want to use as log values, so convert them
        log_list = ["Np","Na","omega0","fbreak","fcut","fscale"]
        for submodel_name in fixedvals.keys():
            params['fixedvals'][submodel_name] = {}
            for name in fixedvals[submodel_name].keys():
                if name in log_list:
                    new_name = "log_"+name
                    params['fixedvals'][submodel_name][new_name] = np.log10(fixedvals[submodel_name][name])
                else:
                    params['fixedvals'][submodel_name][name] = fixedvals[submodel_name][name]
    
    
    
    params['alias'] = eval(str(config.get("params","alias",fallback="{}")))

    params['tdi_lev'] = str(config.get("params", "tdi_lev", fallback='xyz'))
    params['lisa_config'] = str(config.get("params", "lisa_config", fallback='orbiting'))
    params['nside'] = int(config.get("params", "nside"))
    params['lmax'] = int(config.get("params", "lmax"))
    params['model_basis'] = str(config.get("params", "model_basis", fallback='sph'))
    params['tstart'] = float(config.get("params", "tstart", fallback=0))

    ## see if we need to initialize the spherical harmonic subroutines
    sph_check = [sublist.split('-')[0].split('_')[-1] for sublist in params['model'].split('+')]

    # Injection Dict
    inj['doInj']         = int(config.get("inj", "doInj"))
    inj['loadInj']       = int(config.get("inj", "loadInj", fallback=0))
    inj['inj_only']      = int(config.get("inj", "inj_only", fallback=0))
    
    if inj['doInj']:
        ## first see if we are loading the injection
        if inj['loadInj']:
            if inj['inj_only']:
                raise ValueError("Both loadInj and inj_only flags are set to True. This won't accomplish anything...")
            inj['injdir'] = str(config.get("inj", "injdir"))
            ## get the already-generated injection dict
            with open(inj['injdir'] + '/config.pickle', 'rb') as paramfile:
                ## things are loaded from the pickle file in the same order they are put in
                loaded_params = pickle.load(paramfile)
                loaded_inj = pickle.load(paramfile)
            ## for this to work, all params that impact the data/response times/frequencies/types can't change
            ## but you can change e.g., recovery model, sampler, etc.
            required_immutable = ['fmin','fmax','dur','seglen','fs','nside','tstart','lisa_config','tdi_lev','datatype']
            requirements_violated = [requirement for requirement in required_immutable if params[requirement] != loaded_params[requirement] ]
            if len(requirements_violated)>0:
                raise ValueError("Loaded injection is incompatible with specified configuration due to mismatches in the following config settings: {}".format(requirements_violated))
            ## update the injection dictionary with the loaded one
            inj |= loaded_inj
            ## reset the top-level injection flags to their original state
            inj['loadInj'] = True
            inj['inj_only'] = False
        
        ## otherwise make a new one
        else:
            inj['injection'] = str(config.get("inj", "injection"))
            
            ## get the injection basis
            inj['inj_basis'] = str(config.get("inj", "inj_basis", fallback='sph'))
            
            ## get the injection truevals, passed as a dict
            truevals = eval(str(config.get("inj","truevals")))
            ## enforce that the keys of the truevals dict correspond to the injected models
            ## at this point we will only make sure that all the keys are in the injection string
            ## (not all injections will have truevals; e.g., population injections)
            inj_component_names = inj['injection'].split('+')
            if not np.all([key in inj_component_names for key in truevals.keys()]):
                raise ValueError("Truevals dictionary has an invalid key. Truevals must be provided as a nested dictionary with top-level keys corresponding to the injections specified in the 'injection' parameter.")
            
            ## injection per-component multithreading
            inj['parallel_inj'] = int(config.get("inj", "parallel_inj", fallback=0))
            
            ## if parallel_inj is True but there is only one component, set parallel_inj to False
#            if len(inj_component_names) == 1 and inj['parallel_inj']:
#                inj['parallel_inj'] = 0
            
            if inj['parallel_inj']:
                inj['inj_nthread'] = int(config.get("inj", "inj_nthread", fallback=len(inj_component_names)))
                inj['response_nthread'] = int(config.get("inj", "response_nthread", fallback=1))
                ## give preference to response multi-threading
                if inj['inj_nthread'] > 1 and inj['response_nthread'] > 1:
                    print("Warning: you have set both inj_nthread and response_nthread > 1.")
                    print("The Multiprocessing package does not allow workers to spawn additional workers.")
                    print("Giving precedence to response multiprocessing, setting inj_nthread=1.")
                    inj['inj_nthread'] = 1
                elif inj['inj_nthread'] == 1 and inj['response_nthread'] == 1:
                    inj['parallel_inj'] = 0
                    
            else:
                inj['inj_nthread'] = 1
            
            ## build the truevals dict
            ## some quantities we want to use as log values, so convert them
            inj['truevals'] = {}
            log_list = ["Np","Na","omega0","fbreak","fcut","fscale"]
            for component_name in truevals.keys():
                inj['truevals'][component_name] = {}
                for name in truevals[component_name].keys():
                    if name in log_list:
                        new_name = "log_"+name
                        inj['truevals'][component_name][new_name] = np.log10(truevals[component_name][name])
                    else:
                        inj['truevals'][component_name][name] = truevals[component_name][name]
        
        ## add injections to the spherical harmonic check if needed
        sph_check = sph_check + [sublist.split('-')[0].split('_')[-1] for sublist in inj['injection'].split('+')]
        
    
        
    ## pop out to set sph flags
    params['sph_flag'] = ('sph' in sph_check) or ('hierarchical' in sph_check)
    ## set sph flag to false if both inj and model basis are pixel
    if params['model_basis']=='pixel' and inj['inj_basis']=='pixel':
        params['sph_flag'] = False
    
    ## some final flag, injection parameter setting if we aren't loading the Injection directly
    if inj['doInj'] and not inj['loadInj']:
        inj['sph_flag'] = np.any([(item not in ['noise','isgwb']) for item in sph_check])
        ## similarly, set inj sph flag to False if we're doing pixel basis injections
        if inj['inj_basis']=='pixel':
            inj['sph_flag'] = False
        ## set pop flag if spatial and/or spectral injection is a population
        inj['pop_flag'] = ('population' in sph_check) or ('population' in [sublist.split('-')[0].split('_')[0] for sublist in inj['injection'].split('+')])
        
        
        if inj['sph_flag']:
            try:
                inj['inj_lmax'] = int(config.get("inj", "inj_lmax"))
            except configparser.NoOptionError as err:
                if params['sph_flag']:
                    print("Performing a spherical harmonic basis injection and inj_lmax has not been specified. Injection and recovery will use same lmax (lmax={}).".format(params['lmax']))
                    inj['inj_lmax'] = params['lmax']
                else:
                    print("You are trying to do a spherical harmonic injection, but have not specified lmax.")
                    if 'lmax' in params.keys():
                        print("Warning: using analysis lmax parameter for inj_lmax, but you are not performing a spherical harmonic analysis.")
                        inj['inj_lmax'] = params['lmax']
                    else:
                        raise err
        
        
        ## NB -- will have to change this structure to allow pop-based recovery models. But it's a good start.
        if inj['doInj'] and inj['pop_flag']:
            inj['popdict'] = eval(str(config.get("inj","population_params",fallback='None')))
            ## make sure every injection population component has a corresponding entry
            inj_pop_component_names = [cmn for cmn in inj_component_names if 'population' in cmn]
            for cmn in inj_pop_component_names:
                if cmn not in inj['popdict'].keys():
                    raise KeyError("Population injection '{}' does not have a corresponding entry in the population_params dict.".format(cmn))
            ## step through the (possibly multiple) populations and tweak formatting for the delimiters
            ## also enforce the required keys and substitute defaults if optional setting isn't given
            required_keys = ['popfile','columns','delimiter']
            pop_defaults = {'snr_cut':7,'name':None}
            for key in inj['popdict'].keys():
                ## make sure it corresponds to an injection
                if key not in inj_component_names:
                    raise KeyError("Population '{}' not in injection. Top-level keys for the population_params dict must correspond to an injection.".format(key))
                ## enforce required keys
                for rk in required_keys:
                    if rk not in inj['popdict'][key].keys():
                        raise KeyError("population_params dict missing required key: '{}'".format(rk))
                ## set defaults
                for dk in pop_defaults.keys():
                    if dk not in inj['popdict'][key].keys():
                        print("No value found for populations parameter '{}' in population_params dict for injection component '{}'. Setting to default ({}).".format(dk,key,pop_defaults[dk]))
                        inj['popdict'][key][dk] = pop_defaults[dk]   
                ## formatting
                if inj['popdict'][key]['delimiter'] == 'space':
                    inj['popdict'][key]['delimiter'] = ' '
                elif inj['popdict'][key]['delimiter'] == 'tab':
                    inj['popdict'][key]['delimiter'] == '\t'



    # some run parameters
    params['out_dir']            = str(config.get("run_params", "out_dir"))

    params['doPreProc']          = int(config.get("run_params", "doPreProc", fallback=0))
    params['input_spectrum']     = str(config.get("run_params", "input_spectrum", fallback='data_spectrum.npz'))
    params['projection']         = str(config.get("run_params", "projection", fallback='E'))
    params['FixSeed']            = int(config.get("run_params", "FixSeed", fallback=0))
    if params['FixSeed']:
        params['seed']               = int(config.get("run_params", "seed"))
    nthread                      = int(config.get("run_params", "Nthreads", fallback=1))
    N_GPU                        = int(config.get("run_params", "N_GPU", fallback=0))
    
    
    params['colormap']       = str(config.get("run_params", "colormap", fallback='magma'))    
    
    ## sampler selection
    params['sampler'] = str(config.get("run_params", "sampler"))
    
    ## only numpyro has GPU support
    if N_GPU > 0 and params['sampler']!='numpyro':
        raise ValueError("Only numpyro supports GPU acceleration but N_GPU ({}) > 0 and sampler is {}.".format(N_GPU,params['sampler']))
    
    ## sampler setup and late-time imports to reduce dependencies
    ## dynesty
    if params['sampler'] == 'dynesty':
        from blip.src.dynesty_engine import dynesty_engine
        nlive                   = int(config.get("run_params", "nlive", fallback=800))
        params['sample_method'] = str(config.get("run_params", "sample_method", fallback='rwalk'))
    # nessai
    elif params['sampler'] == 'nessai':
        from blip.src.nessai_engine import nessai_engine
        nlive                        = int(config.get("run_params", "nlive", fallback=2000))
        ## flow tuning
        params['nessai_neurons']     = str(config.get("run_params", "nessai_neurons", fallback='scale_greedy'))
        if params['nessai_neurons']=='manual':
            params['n_neurons']      = int(config.get("run_params", "n_neurons", fallback=20))
        params['reset_flow']         = int(config.get("run_params", "reset_flow", fallback=16))
    ## emcee
    elif params['sampler'] == 'emcee':
        from blip.src.emcee_engine import emcee_engine
        params['Nburn'] = int(config.get("run_params", "Nburn",fallback=1000))
        params['Nsamples'] = int(config.get("run_params", "Nsamples",fallback=1000))
    ## numpyro
    elif params['sampler'] == 'numpyro':
        if nthread > 1:
            os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count={}'.format(nthread)
        from blip.src.numpyro_engine import numpyro_engine
        params['show_progress'] = int(config.get("run_params", "show_progress", fallback=1))
        params['Nburn'] = int(config.get("run_params", "Nburn",fallback=1000))
        params['Nsamples'] = int(config.get("run_params", "Nsamples",fallback=1000))
        
    else:
        raise ValueError("Unknown sampler. Can be 'dynesty', 'emcee', 'numpyro', or 'nessai' for now.")
    # checkpointing (dynesty+nessai only for now)
    if params['sampler']=='dynesty' or params['sampler'] == 'nessai' or params['sampler'] == 'numpyro':
        params['checkpoint']            = int(config.get("run_params", "checkpoint", fallback=0))
        ## numpyro's checkpoint_interval is in number of samples, vs. seconds for dynesty/nessai
        if params['sampler'] == 'numpyro':
            params['checkpoint_at'] = str(config.get("run_params", "checkpoint_at", fallback='end'))
            if params['checkpoint_at'] == 'interval':
                params['checkpoint_interval']   = int(config.get("run_params", "checkpoint_interval", fallback=100))
            params['additional_samples'] = int(config.get("run_params", "additional_samples", fallback=0))
            if params['additional_samples'] == 0:
                params['additional_samples'] = None
        else:
            params['checkpoint_interval']   = int(config.get("run_params", "checkpoint_interval", fallback=3600))

    # Fix random seed
    if params['FixSeed']:
        from blip.tools.SetRandomState import SetRandomState as setrs
        seed = params['seed']
        randst = setrs(seed)
    else:
        if params['checkpoint']:
            raise TypeError("Checkpointing without a fixed seed is not supported. Set 'FixSeed' to true and specify 'seed'.")
        if resume:
            raise TypeError("Resuming from a checkpoint requires re-generation of data, so the random seed MUST be fixed.")
        randst = None

    if not resume:
        # Make directories, copy stuff
        # Make output folder
        subprocess.call(["mkdir", "-p", params['out_dir']])
    
        # Copy the params file to outdir, to keep track of the parameters of each run.
        subprocess.call(["cp", paramsfile, params['out_dir']])
        
        # Initialize lisa class
        lisa =  LISA(params, inj)
        
        ## save the Model and Injection as needed
        
        ## the Injection is massive, so discard the responses we no longer need
        ## saving & discarding now, as opposed to at the end of the run also saves space in the dynesty/nessai checkpoint files.
        if not params['load_data']:
            ## save Injection
            for cmn in lisa.Injection.component_names:
                if hasattr(lisa.Injection.components[cmn],'response_mat'):
                    del lisa.Injection.components[cmn].response_mat
                if hasattr(lisa.Injection.components[cmn],'summ_response_mat'):
                    del lisa.Injection.components[cmn].summ_response_mat
                if hasattr(lisa.Injection.components[cmn],'inj_response_mat'):
                    del lisa.Injection.components[cmn].inj_response_mat
            with open(params['out_dir'] + '/injection.pickle', 'wb') as outfile:
                pickle.dump(lisa.Injection, outfile)
            print("Saved Injection to "+params['out_dir']+"/injection.pickle")
            
            ## also save the injected skymaps for later use
            lisa.Injection.extract_and_save_skymap_data()
            
            ## save generated data
            np.savez_compressed(params['out_dir']+'/simulated_data.npz',timearray=lisa.timearray,h1=lisa.h1,h2=lisa.h2,h3=lisa.h3)
            print("Saved strain time series to "+params['out_dir']+"/simulated_data.npz")
            ## Data generation is complete. Exit if only performing injection and data simulation.
            if inj['inj_only']:
                print("Simulation and generation of LISA data is complete and inj_only flag is set to True. Saving configuration and exiting...")
                # Save parameters as a pickle
                with open(params['out_dir'] + '/config.pickle', 'wb') as outfile:
                    pickle.dump(params, outfile)
                    pickle.dump(inj, outfile)
                
                return
        
        ## the Model tends to be more lightweight
        with open(params['out_dir'] + '/model.pickle', 'wb') as outfile:
            pickle.dump(lisa.Model, outfile)
        print("Saved Model to "+params['out_dir']+"/model.pickle")
        
        # Also save the config so we can load and use our simulated data if the run fails during sampling
        with open(params['out_dir'] + '/config.pickle', 'wb') as outfile:
            pickle.dump(params, outfile)
            pickle.dump(inj, outfile)

        print("Generating sampling engine...")
    else:
        print("Resuming a previous analysis. Reloading data and sampler state...")

    if params['sampler'] == 'dynesty':
        # Create engine
        if not resume:
            # multiprocessing
            if nthread > 1:
                pool = Pool(nthread)
            else:
                pool = None
            engine, parameters = dynesty_engine().define_engine(lisa, params, nlive, nthread, randst, pool=pool)    
        else:
            pool = None
            if nthread > 1:
                print("Warning: Nthread > 1, but multiprocessing is not supported when resuming a run. Pool set to None.")
                ## To anyone reading this and wondering why:
                ## The pickle calls used by Python's multiprocessing fail when trying to run the sampler after saving/reloading it.
                ## This is because pickling the sampler maps all its attributes to their full paths;
                ## e.g., dynesty_engine.isgwb_prior is named as src.dynesty_engine.dynesty_engine.isgwb_prior
                ## BUT the object itself is still e.g. <function dynesty_engine.isgwb_prior at 0x7f8ebcc27130>
                ## so we get an error like
                ## _pickle.PicklingError: Can't pickle <function dynesty_engine.isgwb_prior at 0x7f8ebcc27130>: \
                ##                        it's not the same object as src.dynesty_engine.dynesty_engine.isgwb_prior
                ## See e.g. https://stackoverflow.com/questions/1412787/picklingerror-cant-pickle-class-decimal-decimal-its-not-the-same-object
                ## After too much time and sanity spent trying to fix this, I have admitted defeat.
                ## Feel free to try your hand -- maybe you're the chosen one. Good luck.
                
            engine, parameters = dynesty_engine.load_engine(params,randst,pool)
        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples, logz, logzerr = dynesty_engine.run_engine_with_checkpointing(engine,parameters,params['checkpoint_interval'],checkpoint_file,step=200)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        else:
            t1 = time.time()
            post_samples, logz, logzerr = dynesty_engine.run_engine(engine)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        if nthread > 1:
            engine.pool.close()
            engine.pool.join()
        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
        np.savetxt(params['out_dir'] + "/logz.txt", logz)
        np.savetxt(params['out_dir'] + "/logzerr.txt", logzerr)

    elif params['sampler'] == 'emcee':
        # multiprocessing
        if nthread>1:
            pool=Pool(nthread)
        else:
            pool=None

        # Create engine
        engine, parameters, init_samples = emcee_engine.define_engine(lisa.Model, nlive, randst, pool=pool)
        unit_samples, post_samples = emcee_engine.run_engine(engine, lisa.Model, init_samples,params['Nburn'],params['Nsamples'])

        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/unit_samples.txt",unit_samples)
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)

    elif params['sampler'] == 'numpyro':
        ## create engine
        Ntotal = params['Nsamples']
        if not resume:
            ## without checkpointing, set up the engine to run for Nsamples samples
            ## with checkpointing, set it up to run for checkpoint_interval samples (and set the starting chain to None)
            if params['checkpoint'] and params['checkpoint_at']=='interval':
                Nrun = params['checkpoint_interval']
            else:
                Nrun = Ntotal
            
            ## set initial chain to None
            chain = None
            
            if N_GPU > 0:
                use_gpu = True
            else:
                use_gpu = False
            engine, parameters, rng_key = numpyro_engine.define_engine(lisa, params['Nburn'], Nrun, nthread, params['show_progress'], params['seed'], gpu=use_gpu)
            lisaModel = lisa.Model
            
        else:
            resume_file = params['out_dir']+'/checkpoint.pickle'
            engine, rng_key, chain = numpyro_engine.load_engine(resume_file)
            with open(params['out_dir'] + '/model.pickle', 'rb') as modfile:
                lisaModel = pickle.load(modfile)
            parameters = lisaModel.parameters
            
            ## check to see whether the run has already finished
            if chain is None:
                N_current = 0
            else:
                N_current = len(chain['theta_transformed'][0])
            
            ## additional samples
            if params['additional_samples'] is not None:
                if N_current < params['Nsamples']:
                    print("WARNING: {} additional samples were requested, but the original run isn't finished ({}/{} samples remaining)".format(params['additional_samples'],params['Nsamples'],params['Nsamples']))
                    print("WARNING: Disregarding and continuing the original run...")
                else:
                    print("Run was previously completed and {} additional samples have been requested. Resuming chain sampling from final state...".format(params['additional_samples']))
                    ## new total = old total + additional
                    Ntotal = N_current + params['additional_samples']
                    ## if not sampling in N=checkpoint_interval chunks, set the requested # of samples
                    if params['checkpoint_at'] != 'interval':
                        engine.num_samples = params['additional_samples']
            ## if we're not doing additional samples and the run is over, raise a helpful error.
            elif N_current==params['Nsamples']:
                raise ValueError("Run was resumed but has already finished. If you want to continue adding samples to the chain, specify additional_samples=[N desired] in the run directory params file before resuming.")

        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples = numpyro_engine.run_engine_with_checkpointing(engine,lisaModel,rng_key,chain,checkpoint_file,Ntotal,params['checkpoint_at'],resume=resume)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        else:
            t1 = time.time()
            post_samples = numpyro_engine.run_engine(engine,lisa,rng_key)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
        ## save chain
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
    
    elif params['sampler'] == 'nessai':
        # Create engine
        if not resume:
            engine, parameters, model = nessai_engine().define_engine(lisa, params, nlive, nthread, params['seed'], params['out_dir']+'/nessai_output/',checkpoint_interval=params['checkpoint_interval'])    
        else:
            engine, parameters, model = nessai_engine.load_engine(params,nlive,nthread,params['seed'],params['out_dir']+'/nessai_output/',checkpoint_interval=params['checkpoint_interval'])
        ## run sampler
        if params['checkpoint']:
            checkpoint_file = params['out_dir']+'/checkpoint.pickle'
            t1 = time.time()
            post_samples, logz, logzerr = nessai_engine.run_engine_with_checkpointing(engine,parameters,model,params['out_dir']+'/nessai_output/',checkpoint_file)
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
            np.savetxt(params['out_dir']+'/time_elapsed.txt',np.array([t2-t1]))
        else:
            t1 = time.time()
            post_samples, logz, logzerr = nessai_engine.run_engine(engine,parameters,model,params['out_dir']+'/nessai_output/')
            t2= time.time()
            print("Elapsed time to converge: {} s".format(t2-t1))
            np.savetxt(params['out_dir']+'/time_elapsed.txt',np.array([t2-t1]))

        # Save posteriors to file
        np.savetxt(params['out_dir'] + "/post_samples.txt",post_samples)
    
    else:
        raise TypeError('Unknown sampler model chosen. Only dynesty, nessai, numpyro, & emcee are supported')
    
#    
    ## Safely re-save the config pickle, now with the model parameters
    temp_file = params['out_dir'] + '/config.pickle' + ".temp"
    with open(temp_file, "wb") as outfile:
        pickle.dump(params, outfile)
        pickle.dump(inj, outfile)
        pickle.dump(parameters, outfile)
    shutil.move(temp_file, params['out_dir'] + '/config.pickle')
       
    print("\nMaking posterior Plots ...")
    ## reload the Model and Injection if needed
    if not resume:
        plotting_Model = lisa.Model
        plotting_Injection = lisa.Injection
    else:
        ## grab the model and injection
        with open(params['out_dir'] + '/model.pickle', 'rb') as modelfile:
            plotting_Model = pickle.load(modelfile)
        if not params['load_data']:
            with open(params['out_dir'] + '/injection.pickle', 'rb') as injectionfile:
                plotting_Injection = pickle.load(injectionfile)
    
    if not params['load_data']:
        cornermaker(post_samples, params, parameters, inj, plotting_Model, Injection=plotting_Injection)
    else:
        cornermaker(post_samples, params, parameters, inj, plotting_Model)
    if plotting_Model.Npar >= 10:
        print("\n")
        print("WARNING: High Model N_parameters detected ({}). Corner plot may be crowded. Try running plotmaker.py and specifying the 'cornersplit' argument via".format(plotting_Model.Npar))
        print("python3 ./blip/tools/plotmaker.py {} --cornersplit type".format(params['out_dir']))
        print("or")
        print("python3 ./blip/tools/plotmaker.py {} --cornersplit submodel".format(params['out_dir']))
        print("\n")
    if not params['load_data']:
        fitmaker(post_samples, params, parameters, inj, plotting_Model, plotting_Injection)
    else:
        fitmaker(post_samples, params, parameters, inj, plotting_Model)

    ## make a map if there is a map to be made
    if np.any([plotting_Model.submodels[sm_name].has_map for sm_name in plotting_Model.submodel_names]):
        if 'healpy_proj' in params.keys():
            mapmaker(post_samples, params, parameters, plotting_Model, coord=params['healpy_proj'], cmap=params['colormap'])
        else:
            mapmaker(post_samples, params, parameters, plotting_Model, cmap=params['colormap'])
        
    

if __name__ == "__main__":

    if len(sys.argv) != 2:
        if sys.argv[2] == 'resume':
            blip(sys.argv[1],resume=True)
        else:
            raise ValueError('Provide (only) the params file as an argument')
    else:
        blip(sys.argv[1])
